{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with large datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and process static data\n",
    "This Section of the notebook \n",
    "1. downloads static data sources, \n",
    "2. loads them into memory, \n",
    "3. cleans that data then saves a cleaned data product, which may be used lateron for some analysis.\n",
    "\n",
    "### Download files from on-line host.\n",
    "Including this step in the pipeline ensures that the data processing steps are reproducible. \n",
    "This will save the severe headache that comes from trying to share a particular cleaned data products.\n",
    "\n",
    "The script below will:\n",
    "1. Check if the data directory exists and clear it if it does.\n",
    "2. Create a new data directory.\n",
    "3. Download a list of specified data files from an online source into the directory.\n",
    "\n",
    "This approach ensures that everyone working on this project has access to the same data in its original form, \n",
    "facilitating consistent results across different environments.\n",
    "\n",
    "This indicates that we will use bash scripts in the first cell.\n",
    "Using Bash for file operations, like downloading data, offers simplicity and efficiency, particularly for straightforward tasks. \n",
    "Bash scripts are concise, fast for file system operations, and seamlessly integrate with the Unix/Linux environment, making them ideal for basic tasks like creating directories or downloading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the code cell as Bash\n",
    "%%bash\n",
    "\n",
    "# Define the directory where data will be stored\n",
    "DATA_DIR=\"../data/camels/\"\n",
    "\n",
    "# Remove the data directory if it already exists to ensure a fresh start\n",
    "if [ -d \"$DATA_DIR\" ]; then rm -Rf $DATA_DIR; fi\n",
    "\n",
    "# Create a new data directory\n",
    "mkdir $DATA_DIR\n",
    "\n",
    "# List of filenames to be downloaded\n",
    "filenames=(camels_clim.txt camels_geol.txt camels_hydro.txt camels_name.txt camels_soil.txt camels_topo.txt camels_vege.txt)\n",
    "\n",
    "# Loop through each file and download it to the data directory\n",
    "for filename in ${filenames[@]}\n",
    "do \n",
    "    wget -O \"${DATA_DIR}${filename}\" \"https://gdex.ucar.edu/dataset/camels/file/${filename}\"\n",
    "done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share the Processing Code, Not Just the Processed Data\n",
    "\n",
    "Sharing the code used for data processing, rather than just the processed data, is crucial for ensuring reproducibility, especially in data science. While this approach might become challenging with very large datasets, it's essential for maintaining transparency and allowing others to understand and replicate your workflow. By sharing code, you provide insights into how raw data is transformed, cleaned, and made ready for analysis, which is invaluable for collaborative projects and scientific research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filenames to be loaded\n",
    "filenames = [\"camels_clim.txt\", \"camels_geol.txt\", \"camels_hydro.txt\",\n",
    "             \"camels_name.txt\", \"camels_soil.txt\", \"camels_topo.txt\", \"camels_vege.txt\"]\n",
    "\n",
    "# Dictionary to store DataFrames for each file\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each file, read it into a DataFrame, and store it in the dictionary\n",
    "for filename in filenames:\n",
    "    with open(f\"../data/camels/{filename}\", \"r\") as f:\n",
    "        # Read the file using pandas, with ';' as the separator and 'gauge_id' as the index column\n",
    "        dfs[filename] = pd.read_csv(f, sep=\";\", index_col=\"gauge_id\")\n",
    "\n",
    "# Concatenate all DataFrames along the columns\n",
    "df = pd.concat([dfs[filename] for filename in filenames], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Text Data and NaN Values\n",
    "\n",
    "Once we consolidate all our data into a single DataFrame, we often encounter text data and NaN (Not a Number) values. Depending on our analysis goals, these may not be useful. For our current example, we require a dataset with complete information, meaning no missing values (NaNs), and our analysis will focus on continuous numerical data. Therefore, it's important to identify and appropriately handle text and NaN values to prepare our dataset for further analysis.\n",
    "\n",
    "Let's start by taking a preliminary look at our DataFrame to understand its structure and the nature of the data it contains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame to inspect its contents\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data for Analysis\n",
    "\n",
    "In data analysis, the treatment of NaN (Not a Number) values depends on the context and objectives of your study. While NaNs can be acceptable or even meaningful in certain scenarios, they might not be suitable for others. In our hypothetical analysis, we require a dataset without any missing values. Therefore, we will remove columns containing NaN values to ensure our dataset is complete and ready for analysis.\n",
    "\n",
    "This step is crucial for maintaining the integrity and reliability of our analysis, as missing data can lead to biased or inaccurate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with any NaN values from the DataFrame\n",
    "df = df.dropna(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Data Cleaning: Removing One Hot Encoded Data\n",
    "\n",
    "In many analytical scenarios, categorical data represented as strings or One Hot encoded data can be quite useful. However, for our specific analysis, we need a dataset consisting solely of numerical values. Therefore, we will identify and remove columns that contain string data, which often represent categorical variables.\n",
    "\n",
    "This step is crucial for aligning our dataset with the requirements of our analysis, ensuring that the data is in the correct format for the statistical or machine learning methods we plan to apply.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold the names of columns to be dropped\n",
    "drop_these_columns = []\n",
    "\n",
    "# Iterate over each column in the DataFrame\n",
    "for camels_data_column in df.columns.values:\n",
    "    # Check if the first value in the column is of string type\n",
    "    if type(df[camels_data_column].values[0]) == str:\n",
    "        # If it is a string, add the column name to the list\n",
    "        drop_these_columns.append(camels_data_column)\n",
    "\n",
    "# Drop the identified columns from the DataFrame\n",
    "df = df.drop(drop_these_columns, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Cleaned Data with a Unique Filename\n",
    "\n",
    "When sharing and managing data files, especially in a collaborative environment, it's good to avoid confusion caused by multiple versions of the same file. To prevent issues related to version control and ensure traceability, we'll save our cleaned dataset with a unique and descriptive filename. This filename will include the current date and time, along with the initials of the person who processed the data. Such a naming convention makes it easier to track changes over time and understand the lineage of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a timestamp string for the current date and time\n",
    "nowstring = datetime.today().strftime(\"%d-%m-%Y_%H%M\")\n",
    "\n",
    "# Initials of the data processor (change as needed)\n",
    "creator_initials = \"jf\"\n",
    "\n",
    "# Save the DataFrame to a CSV file with a unique name\n",
    "df.to_csv(f\"../data/camels/camels_attributes_cleaned_{nowstring}_{creator_initials}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "Impliment the following sections:\n",
    " - Techniques for handling large datasets (e.g., chunking, streaming).\n",
    " - Data cleaning and preprocessing.\n",
    " - Efficient data storage formats (like HDF5, Parquet).\n",
    " - Use of databases for large datasets (SQL and NoSQL).\n",
    " - Parallel processing and distributed computing basics (e.g., using Dask, Spark).\n",
    "Practical Activities:\n",
    " - Demonstration of different data storage formats and their advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
